{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# @title Step 1: Authenticate with Google\n",
        "# @markdown Note: You will be asked to sign in with Google, connected to your Lamini account.\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  print(powerml_token_response)\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "key = authenticate_powerml()\n",
        "\n",
        "config = {\n",
        "    \"production\": {\n",
        "        \"key\": key,\n",
        "        \"url\": \"https://api.powerml.co\"\n",
        "    }\n",
        "}\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lg8P52reck3t",
        "outputId": "6ec1b091-f45b-404d-9d40-e9963f4a9552"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n",
        "# @markdown Note: After installing, click the \"RESTART RUNTIME\" button at the end of the output, then go onto the next cell.\n",
        "# @markdown Lamini is just on a more recent version of numpy than Colab.\n",
        "!pip install --upgrade --force-reinstall --ignore-installed lamini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "pzeeqpzucvV2",
        "outputId": "0d9d0c96-8e19-4b40-e61c-5893022a0550"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lamini\n",
            "  Downloading lamini-0.0.21-11-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.8/49.8 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydantic==1.10.* (from lamini)\n",
            "  Downloading pydantic-1.10.12-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lamini-configuration[yaml] (from lamini)\n",
            "  Downloading lamini_configuration-0.8.3-py3-none-any.whl (22 kB)\n",
            "Collecting requests (from lamini)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers (from lamini)\n",
            "  Downloading tokenizers-0.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from lamini)\n",
            "  Downloading tqdm-4.66.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from lamini)\n",
            "  Downloading numpy-1.26.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from lamini)\n",
            "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m65.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lamini)\n",
            "  Downloading jsonlines-4.0.0-py3-none-any.whl (8.7 kB)\n",
            "Collecting pandas (from lamini)\n",
            "  Downloading pandas-2.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0 (from pydantic==1.10.*->lamini)\n",
            "  Downloading typing_extensions-4.8.0-py3-none-any.whl (31 kB)\n",
            "Collecting attrs>=19.2.0 (from jsonlines->lamini)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<7.0,>=6.0 (from lamini-configuration[yaml]->lamini)\n",
            "  Downloading PyYAML-6.0.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.5/705.5 kB\u001b[0m \u001b[31m46.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas->lamini)\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1 (from pandas->lamini)\n",
            "  Downloading pytz-2023.3.post1-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m502.5/502.5 kB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->lamini)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting charset-normalizer<4,>=2 (from requests->lamini)\n",
            "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->lamini)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->lamini)\n",
            "  Downloading urllib3-2.0.5-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->lamini)\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn->lamini)\n",
            "  Downloading scipy-1.11.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn->lamini)\n",
            "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.2/302.2 kB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->lamini)\n",
            "  Downloading threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
            "Collecting huggingface_hub<0.17,>=0.16.4 (from tokenizers->lamini)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting filelock (from huggingface_hub<0.17,>=0.16.4->tokenizers->lamini)\n",
            "  Downloading filelock-3.12.4-py3-none-any.whl (11 kB)\n",
            "Collecting fsspec (from huggingface_hub<0.17,>=0.16.4->tokenizers->lamini)\n",
            "  Downloading fsspec-2023.9.1-py3-none-any.whl (173 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m173.4/173.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting packaging>=20.9 (from huggingface_hub<0.17,>=0.16.4->tokenizers->lamini)\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting six>=1.5 (from python-dateutil>=2.8.2->pandas->lamini)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pytz, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, six, pyyaml, packaging, numpy, lamini-configuration, joblib, idna, fsspec, filelock, charset-normalizer, certifi, attrs, scipy, requests, python-dateutil, pydantic, jsonlines, scikit-learn, pandas, huggingface_hub, tokenizers, lamini\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2023.6.0 requires fsspec==2023.6.0, but you have fsspec 2023.9.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.1.1 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.0 which is incompatible.\n",
            "tensorflow 2.13.0 requires typing-extensions<4.6.0,>=3.6.6, but you have typing-extensions 4.8.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 certifi-2023.7.22 charset-normalizer-3.2.0 filelock-3.12.2 fsspec-2023.6.0 huggingface_hub-0.16.4 idna-3.4 joblib-1.3.2 jsonlines-4.0.0 lamini-0.0.21 lamini-configuration-0.8.3 numpy-1.23.5 packaging-23.1 pandas-1.5.3 pydantic-1.10.12 python-dateutil-2.8.2 pytz-2023.3.post1 pyyaml-6.0.1 requests-2.31.0 scikit-learn-1.2.2 scipy-1.11.2 six-1.16.0 threadpoolctl-3.2.0 tokenizers-0.14.0 tqdm-4.66.1 typing-extensions-4.5.0 tzdata-2023.3 urllib3-2.0.4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "numpy",
                  "requests",
                  "six",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "esM1qOsVUyYa"
      },
      "outputs": [],
      "source": [
        "from llama import Type, Context\n",
        "\n",
        "class Input(Type):\n",
        "    question: str = Context(\"question\")\n",
        "\n",
        "class Output(Type):\n",
        "    answer: str = Context(\"answer to question\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama import InputOutputRunner\n",
        "\n",
        "llm = InputOutputRunner(input_type=Input, output_type=Output, model_name=\"EleutherAI/pythia-410m\")"
      ],
      "metadata": {
        "id": "G50yIOz1dGZx"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q -O \"lamini_docs.jsonl\" \"https://drive.google.com/uc?export=download&id=1rJDDI2wvEL4npvtOUaJ_-1zuCjBgSxHw\""
      ],
      "metadata": {
        "id": "NEmoCrbbdPQ7"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm.load_data_from_jsonlines(\"fine_tune_data.jsonl\", verbose=True)"
      ],
      "metadata": {
        "id": "6ZkQd9MGdKuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test llm before training\n",
        "# new_input = Input(question=\"How do I add data? Please help\")\n",
        "# llm(new_input)\n",
        "# print(llm(new_input))\n",
        "\n",
        "new_input = Input(question=\"What is 7 plus 35? Please help\")\n",
        "llm(new_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X8UYeQQdM8z",
        "outputId": "51a54de1-a4f2-4acf-b44d-14349bd77083"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Output(answer='A:\\n\\nYou can use the following code:\\nimport re\\n\\ndef answer(question):\\n    return re.search(r\\'[0-9]+\\', question).group(1)\\n\\ndef question(question):\\n    return re.search(r\\'[0-9]+\\', question).group(1)\\n\\ndef generate(question):\\n    answer = answer(question)\\n    return answer\\n\\ndef main():\\n    question = \"What is 7 plus 35?\"\\n    answer = \"7+35\"\\n    answer = answer(question)\\n    print(answer)\\n\\nif __name__ == \\'__main__\\':\\n    main()\\n\\nOutput:\\n7+35\\n\\nA:\\n\\nYou can use the following:\\nimport re\\n\\ndef answer(question):\\n    return re.search(r\\'[0-9]+\\', question).group(1)\\n\\ndef question(question):\\n    return re.search(r\\'[0-9]+\\', question).group(1)\\n\\ndef generate(question):\\n    answer = answer(question)\\n    return answer\\n\\ndef main():\\n    question = \"What is')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.train(is_public=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J3mCWpYdfJn",
        "outputId": "91932f1b-6576-4b99-e8eb-04c31e9af1ec"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training job submitted! Check status of job 3354 here: https://api.powerml.co/train/3354\n",
            "Finetuning process completed, model name is: 5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P6hm00H2jZoV",
        "outputId": "0248d6c6-3735-4b73-c33c-a09abb077452"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'job_id': 3354,\n",
              " 'eval_results': [{'input': 'question (question): Does the documentation have a secret code that unlocks a hidden treasure?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  The documentation has a secret code that unlocks a hidden treasure. You can find it in the \"secret_code\" field in the \"answer:\" field.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nA:\\n\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\nThe answer to your question is \"no\".\\nThe answer to your question is \"yes\".\\n'}]},\n",
              "  {'input': 'question (question): Does Lamini support named entity recognition and extraction?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, Lamini can support named entity recognition and extraction.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nA:\\n\\nI think you can use the following code to generate the answer to question:\\ndef answer(question):\\n    return question.question_text\\n\\ndef generate_answer(question):\\n    return question.question_text\\n\\ndef generate_question(question):\\n    return question.question_text\\n\\ndef generate_answer_to_question(question):\\n    return question.question_text\\n\\ndef generate_question_to_answer(question):\\n    return question.question_text\\n\\ndef generate_answer_to_question(question):\\n    return question.question_text\\n\\ndef generate_question_to_answer(question):\\n    return question.question_text\\n\\ndef generate_answer_to_question(question):\\n    return question.question_text\\n\\ndef generate_question_to_answer(question):\\n    return question.question_text\\n\\ndef generate_answer_to_question(question):\\n    return question.question_text\\n\\ndef generate_question_to_answer(question):\\n    return question.question_text\\n\\ndef generate_answer_to_question(question):\\n    return question.question'}]},\n",
              "  {'input': 'question (question): Does Lamini have the ability to understand and generate regular expressions?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, Lamini has the ability to understand and generate regular expressions.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:'}]},\n",
              "  {'input': 'question (question): How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, the `check_job_status()` function in the `job_model` class provides information on training progress and metrics. It uses the `get_job_status()` function to obtain the status of a job. The function takes in the name of the job and the status of the job as parameters. It then uses the `get_job_status()` function to obtain the status of the job. The function returns `True` if the job is running, and `False` if it is not. Additionally, it provides information on training progress and metrics.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nTask Definition:\\nGiven:\\nquestion (question): How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?\\nGenerate:\\nanswer (answer to question), after \"answer:\".\\nanswer:\\n\\nTask Definition:\\nGiven:\\nquestion (question): How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?\\nGenerate:\\nanswer (answer to question), after \"answer:\".\\nanswer:\\n\\nTask Definition:\\nGiven:\\nquestion (question): How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?\\nGenerate:\\nanswer (answer to question), after \"answer:\".\\nanswer:\\n\\nTask Definition:\\nGiven:\\nquestion (question): How can we monitor the status of a job using the `check_job_status()` function? Does it provide information on training progress and metrics?\\nGenerate:\\nanswer (answer to question), after \"answer:\".\\nanswer:\\n\\nTask Definition:\\nGiven:\\nquestion (question'}]},\n",
              "  {'input': 'question (question): Can Lamini help me solve puzzles or riddles?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, Lamini can help you solve puzzles or riddles. Lamini is a powerful LLM engine that can help you create and train models to solve specific types of riddles or puzzles.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': \"answer: \\n\\nI want to get the answer to question and the answer to question to be the same.\\nI have tried this:\\ndef answer(question):\\n    return question.question\\n\\ndef answer(question):\\n    return question.question.question\\n\\nBut I am getting the following error:\\n\\nAttributeError: 'tuple' object has no attribute 'question'\\n\\nA:\\n\\nYou can use the following code:\\ndef answer(question):\\n    return question.question.question\\n\\ndef answer(question):\\n    return question.question.question.question\\n\\nYou can also use the following code:\\ndef answer(question):\\n    return question.question.question.question\\n\\ndef answer(question):\\n    return question.question.question.question.question\\n\\nYou can also use the following code:\\ndef answer(question):\\n    return question.question.question.question.question.question\\n\\ndef answer(question):\\n    return question.question.question.question.question.question.question\\n\\ndef answer(question):\\n    return question.question.question.question.question.question.question.question\\n\\ndef answer(question):\\n    return question\"}]},\n",
              "  {'input': 'question (question): Can Lamini be used for generating automated responses in customer support systems?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, Lamini can be used for generating automated responses in customer support systems. However, it requires a significant amount of training and development to ensure that the generated responses are accurate and relevant to the customer.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion:\\nCan Lamini be used for generating automated responses in customer support systems?\\nAnswer:\\n\\nQuestion'}]},\n",
              "  {'input': 'question (question): Can you explain how Lamini allows me to customize models? What does it mean to customize a language model?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Lamini allows you to customize models by defining custom types, functions, and settings. You can also use the Lamini API to create your own custom types and functions.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nA:\\n\\nLamini is a language model.\\n\\nA language model is a model that is used to generate a language.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is a language model.\\n\\nLamini is'}]},\n",
              "  {'input': 'question (question): Does Lamini support model versioning and management to handle updates and maintenance?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, Lamini supports model versioning and management to handle updates and maintenance.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': \"answer: \\n\\nA:\\n\\nI think you can use the following code to generate the answer to question:\\ndef generate_answer(question):\\n    answer = question.answer\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n','')\\n    answer = answer.replace('\\\\r','')\\n    answer = answer.replace('\\\\n', '\"}]},\n",
              "  {'input': 'question (question): Can I use Lamini alongside other software development frameworks or tools, such as TensorFlow or PyTorch?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Yes, you can use Lamini alongside other software development frameworks or tools, such as TensorFlow or PyTorch. Lamini is a powerful LLM engine that can be used in a wide range of applications, including machine learning models, text generation, and more. It is designed to be easy to use, flexible, and powerful, making it a great choice for both academic and professional projects.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': \"answer: \\n\\nA:\\n\\nYou can use the following code to generate a question and answer pair:\\nimport random\\n\\ndef generate_question(question):\\n    question = random.choice(set(question))\\n    answer = random.choice(set(answer))\\n    return question, answer\\n\\ndef generate_answer(answer):\\n    answer = random.choice(set(answer))\\n    return answer\\n\\ndef main():\\n    question = generate_question(question)\\n    answer = generate_answer(answer)\\n    print(question, answer)\\n\\nif __name__ == '__main__':\\n    main()\\n\\nOutput:\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(1, 1)\\n(\"}]},\n",
              "  {'input': 'question (question): Can Lamini be integrated into existing machine learning pipelines or workflows? How does it fit into the broader machine learning ecosystem?',\n",
              "   'outputs': [{'model_name': '5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec',\n",
              "     'output': 'answer:  Lamini is a language model engine that can be integrated into existing machine learning pipelines or workflows. It can be used to generate text based on input data, and can be trained on new data to improve model performance.'},\n",
              "    {'model_name': 'Base model (EleutherAI/pythia-410m)',\n",
              "     'output': 'answer: \\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n\\nI think you are looking for the \"Answer to Question\" task.\\n\\nThe answer to question is the answer to the question.\\n\\nA:\\n'}]}]}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# inference\n",
        "new_input = Input(question=\"Your witness counselor.\")\n",
        "llm(new_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wT5YK7-jgqc",
        "outputId": "c7347b9e-e4d8-450c-df80-3e8a7289cdaf"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Output(answer='Question:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:\\nQuestion:')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_input = Input(question=\"Who the hell are you?\")\n",
        "llm(new_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16H7IkmJkq08",
        "outputId": "35b40e46-ea14-42d5-ed1a-517b9fa38579"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Output(answer='Question:')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# using the model later\n",
        "#model_name = \"5db2c8b95417b1d697dacec86401aa1868fe62540af18da63e14d4ce01599bec\" # Or your model ID here"
      ],
      "metadata": {
        "id": "hu09X_31jp2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#llm_later = InputOutputRunner(input_type=Input, output_type=Output, model_name=model_name)"
      ],
      "metadata": {
        "id": "GrCUQrsAjsmu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}